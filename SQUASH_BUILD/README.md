# SQUASH_BUILD

## Folder Structure
```script
- datasets: Used to store datasets and generated indices. One subfolder per dataset.
- logs: Build logs are automatically written here, and we recommend that command line output is also redirected here.
- output: General output area.
- scripts: Contains scripts to generate uniform attributes, as well as queries with a given selectivity level.
- src:
    - attributeset.py: Indexing attribute data
    - base.py: Used for pipelining successive processing steps.
    - dataset.py: Calculates KLT transformation matrix for a dataset/partition.
    - transformed.py: Performs KLT, and contains several utility functions (e.g., to perform random reads)
    - qsession.py: Top-level class coordinating the execution of other components.
    - vaqindex.py: Main indexing class. Performs SQ indexing of data (e.g., energy calculations, non-uniform bit allocations, boundary initialization, Lloyd's algorithm), as well as binary quantization.
- auto_build.sh: Used for automating multiple build jobs.
- param_template.json: Parameter template for use with auto_build.sh
- runprvd.py: Used to print detailed information about a given vector ID.
- sb_auto_runner.py: Runner script for use with auto_build.sh
- sb_runner.py: Standard runner script, used for individual build jobs.
```

## Dependencies
The following package versions were used for SQUASH_BUILD.
```script
- python==3.9
- numpy==1.24.3
- k_means_constrained==0.7.3
- bitarray==2.5.1
- boto3==1.34.82
```

## Instructions
We recommend running the SQUASH_BUILD module on a moderately-sized server for improved efficiency. A range of instance types with different cost/performance profiles are available via AWS EC2 (https://aws.amazon.com/ec2/pricing/).
- Once a given dataset XX has been downloaded, place it in the SQUASH_BUILD/datasets/XX directory. Only the raw dataset (fvecs file) is required at this stage (e.g. sift1m, no extensions.)
- If attributes are not provided with a given dataset, uniform attributes can be generated using scripts/generate_attributes.py
- Next, sb_runner.py should be configured accordingly. This includes providing dataset information (# vectors, # dimensions, # attributes, # partitions), as well as SQUASH-specific parameters (e.g., bit budget). The max_workers parameter to ProcessPoolExecutor can be used to dictate the number of partitions to be processed in parallel, following the initial Constrained K Means phase. This is dependent on the available compute resources.
    - As mentioned above, sb_auto_runner.py and param_template.json can be used in conjunction with auto_build.sh if multiple successive build jobs are required (e.g., for different datasets or for parameter testing). Create a unique directory for each build job, and copy param_template.json (configured accordingly) into each.
- Run sb_runner.py. The datasets/XX directory will be populated with several files shared amongst partitions (e.g. XX.ptnrvars.npz for partitioner variables), as well as partitions/0, partitions/1, ... directories containing the local quantization indices for each partition.
- Optionally, queries of a given selectivity level can also be generated. 
    - This can be done using scripts/generate_querydata.py. 
    - These can be split over a specified number of query-parallel 'allocators', as discussed in the paper.
    - The generate_querydata.py script creates allocators/0, allocators/1, ... directories within the given dataset folder, each containing the queries for a given allocator.
    - Note that the selectivity criteria assume that the uniform attribute generation script has been used. 
- Finally, the generated files should be uploaded to AWS, for use with SQLAYER
    - Run sqfiles_reduced.sh to generate a minimal zip of the files to be stored in EFS. Note that the extra files not included here are intermediate files generated by the build process.
    - We recommend creating an S3 bucket (e.g. squash_data_bucket) containing two folders, one being a holding area for file zips, and one for datasets themselves (i.e., the files directly accessed from S3 at runtime)
    - Run squash_aws_cli_upload.py to upload the zip to S3. This will later be transferred to EFS and unzipped for use with SQLAYER. 
    - Run dataset_transfer to upload the allocators/partitions directories for a given dataset, as well as qavars.npz, to squash_data_bucket/datasets/XX.
